Huang et al.
considering costs from a local window of the map, as previous CNN-based optical
flow estimation methods do. Even for estimating & single source pixel's accurate
displacement, it is beneficial to take its contextual source pixels' cost maps into
consideration:
To tackle this challenging problem, we propose & transformer-based cost vol-
ume encoder that encodes the whole cost volume into a cost memory. Our cost
volume encoder consists of three steps: 1) cost map pathification, 2) cost patch
token embedding, and 3) cost memory encoding: We elaborate the details of the
three steps as follows.
Cost map patchifcation. Following existing vision transformers, we patchify
the cost map Mx € RHxW of each source pixel x with strided convolutions to ob -
tain a sequence of cost patch embeddings. Specifically; given an HxW cost map,
we first pad zeros at its right and bottom sides to make its width and height mul-
tiples of 8. The padded cost map is then transformed by & stack of three stride-2
convolutions followed by ReLU into a feature map Fx € RTH/87xTW/8] xDp _ Each
feature in the feature map stands for an 8 x 8 patch in the input cost map. The
three convolutions have output channels of Dpl4, Dp/2, Dp, respectively.
Patch Feature Tokenization via Latent Summarization. Although the
patchification results in a sequence of cost patch feature vectors for each source
pixel, the number of such patch features is still large and hinders the efficiency
of information propagation among different source pixels. Actually; a cost map
is highly redundant because only a few high costs are most informative. To
obtain more compact cost features, we further summarize the patch features
Fx of each source pixel X via K latent codewords C € RKxD Specifically;
the latent codewords query each source pixels cost-patch features to further
summarize each cost map into K latent vectors of D dimensions via the dot -
product attention mechanism. The latent codewords C € RKxD are randomly
initialized; updated via back-propagation; and shared across all source pixels.
The latent representations Tx for summarizing Fx are obtained as
Kx Conv1x1 (Concat(Fx, PE)) =
Vx = Conv1x1 (Concat(Fx, PE)) , (1)
Tx Attention(C,Kx, Vx)
Before projecting the cost-patch features Fx to obtain keys Kx and values X)
the patch features are concatenated with a sequence of positional embeddings
Given a 2D position P, we encode it into a positional
embedding of length Dp following COTR [27]. Finally; the cost map of the source
pixel x can be summarized into K latent representations Tx RKxD by con-
ducting multi-head dot-product attention with the queries, keys, and values .
Generally, K x D < H x W and the latent summarizations Tx therefore pro-
vides more compact representations than each H x W cost map for each source
pixel x. For all source pixels in the image, there are 9 total of (H x W) 2D cost
maps. Their summarized representations can consequently be converted into
latent 4D cost volume T € RHxWxKxD_