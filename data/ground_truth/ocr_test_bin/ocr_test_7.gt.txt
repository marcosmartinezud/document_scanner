8
Huang et al.
appearance affinities between different source pixels into SS-SelfAttention via concatenating the source image's context features t with the cost tokens when generating queries and keys. We call this layer inter-cost-map self-attention layer as it is responsible to propagate information of cost volume across different source pixels.
The above self-attention operations' parameters are shared across different groups and they are sequentially operated to form the proposed alternate-group attention layer. By stacking the alternate-group transformer layer multiple times, the latent cost tokens can effectively exchange information across source pixels and across latent representations to better encode the 4D cost volume. In this way, our cost volume encoder transforms the H x W x H x W 4D cost volume to HxWxK latent tokens of length D. We call the final H x W x K tokens as the cost memory, which is to be decoded for optical flow estimation.
3.3 Cost Memory Decoder for Flow Estimation
Given the cost memory encoded by the cost volume encoder, we propose a cost memory decoder to predict optical flows. Since the original resolution of the input image is HI X W1, we estimate optical flow at the H x W resolution and then upsample the predicted flows to the original resolution with a learnable convex upsampler [47]. However, in contrast to previous vision transformers that seek abstract semantic features, optical flow estimation requires recovering dense correspondences from the cost memory. Inspired by RAFT [47], we propose to use cost queries to retrieve cost features from the cost memory and iteratively refine flow predictions with a recurrent attention decoder layer. Cost memory aggregation. For predicting the flows of the H x W source pixels, we generate a sequence of (H x W) cost queries, each of which is re- sponsible for estimating the flow of a single source pixel via co-attention on the cost memory. To generate the cost query Qx for a source pixel x, we first compute its corresponding location in the target image given its current esti- mated flow f(x) as p = x + f(x). We then retrieve a local 9 x 9 cost-map patch qx = Crop9x9 (Mx, p) by cropping costs inside the 9 x 9 local window centered at p on the cost map Mx. The cost query Qx is then formulated based on the features FFN (qx) that encoded from the local costs qx and p's positional embedding PE(p), which can aggregate information from source pixel x's cost memory Tx via cross-attention,
Qx
Kx
=
=
FFN (FFN(qx) + PE(p)),
FFN (Tx), Vx = FFN (Tx),
Cx = Attention (Qx, Kx, Vx).
(4)
The cross-attention summarizes information from the cost memory for each source pixel to predict its flow. As Qx is dynamically updated in terms of the fed position at each iteration, we call it as dynamic positional cost query. We note that keys and values can be generated at the beginning and re-used in subsequent iterations, which saves computation as a benefit of our recurrent decoder.