
2.2. NAÏVE BAYES
19
As in the categorical distribution, the parameter ; can be interpreted as a probabil- ity: specifically, the probability that any given token in the document is the word j. The multinomial distribution involves a product over words, with each term in the product equal to the probability ;, exponentiated by the count x. Words that have zero count play no role in this product, because = 1. The term B(x) is called the multinomial coefficient. It doesn't depend on 4, and can usually be ignored. Can you see why we need this term at all?9
The notation p(x | y;) indicates the conditional probability of word counts a given label y, with parameter 4, which is equal to Pmult(x; y). By specifying the multinomial distribution, we describe the multinomial Naïve Bayes classifier. Why "naïve"? Because the multinomial distribution treats each word token indepen- dently, conditioned on the class: the probability mass function factorizes across the counts, 10
2.2.1 Types and tokens
A slight modification to the generative model of Naïve Bayes is shown in Algorithm 2. Instead of generating a vector of counts of types, x, this model generates a sequence of tokens, w = (w1, W2,..., WM). The distinction between types and tokens is critical: x; € {0, 1, 2,..., M} is the count of word type j in the vocabulary, e.g., the number of times the word cannibal appears; wm EV is the identity of token m in the document, e.g. wm = cannibal.
The probability of the sequence w is a product of categorical probabilities. Algorithm 2 makes a conditional independence assumption: each token wm is independent of all other tokens wm conditioned on the label y). This is identical to the "naïve" independence assumption implied by the multinomial distribution, and as a result, the optimal parame- ters for this model are identical to those in multinomial Naïve Bayes. For any instance, the probability assigned by this model is proportional to the probability under multinomial Naïve Bayes. The constant of proportionality is the multinomial coefficient B(x). Because B(x) 1, the probability for a vector of counts x is at least as large as the probability for a list of words w that induces the same counts: there can be many word sequences that correspond to a single vector of counts. For example, man bites dog and dog bites man correspond to an identical count vector, {bites: 1, dog: 1, man: 1}, and B(x) is equal to the total number of possible word orderings for count vector x.
"Technically, a multinomial distribution requires a second parameter, the total number of word counts in x. In the bag-of-words representation is equal to the number of words in the document. However, this parameter is irrelevant for classification.
10 You can plug in any probability distribution to the generative story and it will still be Naïve Bayes, as long as you are making the "naïve" assumption that the features are conditionally independent, given the label. For example, a multivariate Gaussian with diagonal covariance is naïve in exactly the same sense.
Under contract with MIT Press, shared under CC-BY-NC-ND license.